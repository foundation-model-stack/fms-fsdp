{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65cc8fc0",
   "metadata": {},
   "source": [
    "- To download, evaluate and compare the HF's smolLM-135M model and our trained one\n",
    "\n",
    "- conda env: \n",
    "CONDA_ENV_PATH=\"/proj/data-eng/granite_docling/conda-envs/fms-fsdp-env\" # NEW env: pip list (but install with -e): ibm-fms 1.0.0     /proj/data-eng/granite_docling/foundation-model-stack\n",
    "\n",
    "\n",
    "CONDA_ENV_PATH=\"/proj/data-eng/fsdp/train_env\" # ab.study env -> use fms which may not support BOTH KVQ and MLP fused! at /proj/data-eng/fsdp/foundation-model-stack/\n",
    "\n",
    "- ref: \n",
    "    + SmolLM2-135M: https://huggingface.co/HuggingFaceTB/SmolLM2-135M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c070fca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "                         /proj/data-eng/finetune/conda-envs/dpk\n",
      "                         /proj/data-eng/finetune/conda-envs/env-fms-hf-tuning\n",
      "                         /proj/data-eng/finetune/conda-envs/instruct-tune-env\n",
      "                         /proj/data-eng/fms-hf-tuning/train_env\n",
      "                      *  /proj/data-eng/granite_docling/conda-envs/fms-fsdp-env\n",
      "                         /proj/data-eng/replaybuffer/07-conda-env/fms-hf-tuning-env\n",
      "                         /proj/data-eng/replaybuffer/07-conda-env/fms-hf-tuning-env2\n",
      "                         /proj/data-eng/replaybuffer/07-conda-env/fms-hf-tuning-yashavi-env-clone1\n",
      "                         /proj/data-eng/replaybuffer/07-conda-env/open-instruct-env\n",
      "                         /proj/data-eng/replaybuffer/07-conda-env/open-instruct-fastmoe-env\n",
      "                         /proj/data-eng/replaybuffer/07-conda-env/open-instruct-gg-env\n",
      "base                     /u/xdang/miniconda3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa75cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/data-eng/granite_docling/conda-envs/fms-fsdp-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/proj/data-eng/granite_docling/conda-envs/fms-fsdp-env/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/proj/data-eng/granite_docling/conda-envs/fms-fsdp-env/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Download:\n",
    "'''\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "local_dir = \"/proj/data-eng/granite_docling/experiments/SmolLM2-135M\"\n",
    "\n",
    "# Download and save the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.save_pretrained(local_dir)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "model.save_pretrained(local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fae121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "local_dir = \"/proj/data-eng/granite_docling/experiments/SmolLM2-135M\"\n",
    "\n",
    "# Download and save the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.save_pretrained(local_dir)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "model.save_pretrained(local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffcd6a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Prompt: Gravity is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/data-eng/granite_docling/conda-envs/fms-fsdp-env/lib/python3.12/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t -> Response by `SmolLM2-135M`: Gravity is the force that holds the Earth and the Moon together.\n",
      "\n",
      "The Moon is a\n",
      "\t -> Response by `step_100000_ckp`: Gravity is the force that holds the Earth together. It is the force that keeps the Earth in orbit\n",
      "\t -> Response by `step_1900000_ckp`: Gravity is a measure of the resistance of a fluid to flow. The SI unit of pressure is the\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Do inference\n",
    "'''\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def run_inference(local_dirs:list, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Load local models and tokenizers from a local dir and run inference on a given prompt using CPU.\n",
    "\n",
    "    Args:\n",
    "        local_dir (str): Path to the directory where the model and tokenizer are stored.\n",
    "        prompt (str): The input prompt to generate text from.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n== Prompt: {prompt}\")\n",
    "    for local_dir in local_dirs:\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "        model = AutoModelForCausalLM.from_pretrained(local_dir)\n",
    "        \n",
    "        model_name = os.path.basename(os.path.normpath(local_dir))\n",
    "\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        try:\n",
    "            outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "            # outputs = model.generate(inputs,pad_token_id=tokenizer.eos_token_id,max_new_tokens=200)\n",
    "        except Exception as gen_err:\n",
    "                print(f\"\\t⚠️ Skipped generation for `{model_name}` due to error: {gen_err}\")\n",
    "                continue\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\t -> Response by `{model_name}`: {response}\")\n",
    "\n",
    "evaluated_models = [\n",
    "    \"/proj/data-eng/granite_docling/experiments/SmolLM2-135M\",\n",
    "    \"/proj/data-eng/granite_docling/experiments/llama165m_granite4tiktoken-2T-pdbs2-8node/hf_model/step_100000_ckp\",\n",
    "    \"/proj/data-eng/granite_docling/experiments/llama165m_granite4tiktoken-2T-pdbs2-8node/hf_model/step_1900000_ckp\"\n",
    "]\n",
    "\n",
    "run_inference(evaluated_models,\"Gravity is\")\n",
    "\n",
    "# run_inference(\"/proj/data-eng/granite_docling/experiments/SmolLM2-135M\",\"Gravity is\")\n",
    "# run_inference(\"/proj/data-eng/granite_docling/experiments/llama165m_granite4tiktoken-2T-pdbs2-8node/hf_model/step_100000_ckp\",\"Gravity is\")\n",
    "# run_inference(\"/proj/data-eng/granite_docling/experiments/llama165m_granite4tiktoken-2T-pdbs2-8node/hf_model/step_1900000_ckp\",\"Gravity is\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72294cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Prompt: Explain how photosynthesis works in simple terms.\n",
      "\t -> Response by `SmolLM2-135M`: Explain how photosynthesis works in simple terms.\n",
      "\n",
      "Photosynthesis is the process by which plants convert light\n",
      "\t -> Response by `step_100000_ckp`: Explain how photosynthesis works in simple terms. What is the role of the chloroplast in photos\n",
      "\t -> Response by `step_1900000_ckp`: Explain how photosynthesis works in simple terms. What is the process of photosynthesis? What are\n",
      "\n",
      "== Prompt: What would happen if gravity on Earth was suddenly cut in half?\n",
      "\t -> Response by `SmolLM2-135M`: What would happen if gravity on Earth was suddenly cut in half?\n",
      "\n",
      "The answer is that the\n",
      "\t -> Response by `step_100000_ckp`: What would happen if gravity on Earth was suddenly cut in half? What would happen if the Earth were\n",
      "\t -> Response by `step_1900000_ckp`: What would happen if gravity on Earth was suddenly cut in half? Would the Earth still be pulled towards\n",
      "\n",
      "== Prompt: Compare DNA and RNA in terms of structure and function.\n",
      "\t -> Response by `SmolLM2-135M`: Compare DNA and RNA in terms of structure and function.\n",
      "\n",
      "DNA is a double helix, which\n",
      "\t -> Response by `step_100000_ckp`: Compare DNA and RNA in terms of structure and function. The structure of the RNA is similar to the\n",
      "\t -> Response by `step_1900000_ckp`: Compare DNA and RNA in terms of structure and function. The DNA is a double helix, and\n",
      "\n",
      "== Prompt: If a train travels 60 miles per hour for 2.5 hours, how far does it travel?\n",
      "\t⚠️ Skipped generation for `SmolLM2-135M` due to error: Input length of input_ids is 23, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\t⚠️ Skipped generation for `step_100000_ckp` due to error: Input length of input_ids is 22, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\t⚠️ Skipped generation for `step_1900000_ckp` due to error: Input length of input_ids is 22, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\n",
      "== Prompt: What is the next number in the sequence: 2, 4, 8, 16, ?\n",
      "\t⚠️ Skipped generation for `SmolLM2-135M` due to error: Input length of input_ids is 23, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\t⚠️ Skipped generation for `step_100000_ckp` due to error: Input length of input_ids is 22, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\t⚠️ Skipped generation for `step_1900000_ckp` due to error: Input length of input_ids is 22, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\n",
      "== Prompt: A farmer has 17 sheep. All but 9 run away. How many are left?\n",
      "\t⚠️ Skipped generation for `SmolLM2-135M` due to error: Input length of input_ids is 20, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\t -> Response by `step_100000_ckp`: A farmer has 17 sheep. All but 9 run away. How many are left? \n",
      "\t -> Response by `step_1900000_ckp`: A farmer has 17 sheep. All but 9 run away. How many are left? (\n",
      "\n",
      "== Prompt: Write a Python function that checks if a number is prime.\n",
      "\t -> Response by `SmolLM2-135M`: Write a Python function that checks if a number is prime.\n",
      "\n",
      "```python\n",
      "def is_\n",
      "\t -> Response by `step_100000_ckp`: Write a Python function that checks if a number is prime. If it is, it prints the prime\n",
      "\t -> Response by `step_1900000_ckp`: Write a Python function that checks if a number is prime. The function should return True if the number\n",
      "\n",
      "== Prompt: Explain what a Python decorator is and give an example.\n",
      "\t -> Response by `SmolLM2-135M`: Explain what a Python decorator is and give an example.\n",
      "\n",
      "Python decorators are a way to\n",
      "\t -> Response by `step_100000_ckp`: Explain what a Python decorator is and give an example.The 10 Best Ways to Get\n",
      "\t -> Response by `step_1900000_ckp`: Explain what a Python decorator is and give an example. You can use it to add a new\n",
      "\n",
      "== Prompt: Fix this buggy Python code:\n",
      "\n",
      "for i in range(10)\n",
      "    print(i)\n",
      "\t⚠️ Skipped generation for `SmolLM2-135M` due to error: Input length of input_ids is 23, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\t -> Response by `step_100000_ckp`: Fix this buggy Python code:\n",
      "\n",
      "for i in range(10)\n",
      "    print(i) # prints \n",
      "\t -> Response by `step_1900000_ckp`: Fix this buggy Python code:\n",
      "\n",
      "for i in range(10)\n",
      "    print(i) # prints \n",
      "\n",
      "== Prompt: Summarize the following paragraph in one sentence: 'The climate crisis has accelerated due to industrial emissions and deforestation...'\n",
      "\t⚠️ Skipped generation for `SmolLM2-135M` due to error: Input length of input_ids is 24, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\t⚠️ Skipped generation for `step_100000_ckp` due to error: Input length of input_ids is 24, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\t⚠️ Skipped generation for `step_1900000_ckp` due to error: Input length of input_ids is 24, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "\n",
      "== Prompt: Paraphrase: 'Artificial intelligence is changing the way businesses operate.'\n",
      "\t -> Response by `SmolLM2-135M`: Paraphrase: 'Artificial intelligence is changing the way businesses operate.'\n",
      "\n",
      "The term 'art\n",
      "\t -> Response by `step_100000_ckp`: Paraphrase: 'Artificial intelligence is changing the way businesses operate.' (p. 7\n",
      "\t -> Response by `step_1900000_ckp`: Paraphrase: 'Artificial intelligence is changing the way businesses operate.' (Source: Wikipedia)\n",
      "\n",
      "\n",
      "\n",
      "== Prompt: List three steps to make a peanut butter and jelly sandwich.\n",
      "\t -> Response by `SmolLM2-135M`: List three steps to make a peanut butter and jelly sandwich.\n",
      "\n",
      "Step 1:\n",
      "\n",
      "\n",
      "\t -> Response by `step_100000_ckp`: List three steps to make a peanut butter and jelly sandwich. First, you need to make a peanut\n",
      "\t -> Response by `step_1900000_ckp`: List three steps to make a peanut butter and jelly sandwich. 1. Cut the peanut butter into\n",
      "\n",
      "== Prompt: Tell me how to reset my Wi-Fi router.\n",
      "\t -> Response by `SmolLM2-135M`: Tell me how to reset my Wi-Fi router.\n",
      "\n",
      "I have a 2.4\n",
      "\t -> Response by `step_100000_ckp`: Tell me how to reset my Wi-Fi router. I have a 3G network and I have\n",
      "\t -> Response by `step_1900000_ckp`: Tell me how to reset my Wi-Fi router. I don't know how to reset it. I\n",
      "\n",
      "== Prompt: Translate 'Good morning, how are you?' into Spanish.\n",
      "\t -> Response by `SmolLM2-135M`: Translate 'Good morning, how are you?' into Spanish.\n",
      "\n",
      "\"Good morning, how\n",
      "\t -> Response by `step_100000_ckp`: Translate 'Good morning, how are you?' into Spanish. The Spanish word for 'good morning'\n",
      "\t -> Response by `step_1900000_ckp`: Translate 'Good morning, how are you?' into Spanish. It's a good way to get the\n",
      "\n",
      "== Prompt: Write a short poem about the moon.\n",
      "\t -> Response by `SmolLM2-135M`: Write a short poem about the moon.\n",
      "\n",
      "The moon is a very important part of our planet\n",
      "\t -> Response by `step_100000_ckp`: Write a short poem about the moon. What is the moon like? What is the moon like?\n",
      "\t -> Response by `step_1900000_ckp`: Write a short poem about the moon. \n",
      "\n",
      "  1. What is the moon? \n",
      "\n",
      "== Prompt: Continue the story: 'The robot looked at the stars and whispered, \"I remember...\"'\n",
      "\t -> Response by `SmolLM2-135M`: Continue the story: 'The robot looked at the stars and whispered, \"I remember...\"'\n",
      "\n",
      "\t -> Response by `step_100000_ckp`: Continue the story: 'The robot looked at the stars and whispered, \"I remember...\"' (\n",
      "\t -> Response by `step_1900000_ckp`: Continue the story: 'The robot looked at the stars and whispered, \"I remember...\"'\n",
      "\n",
      "== Prompt: Describe a futuristic city built under the ocean.\n",
      "\t -> Response by `SmolLM2-135M`: Describe a futuristic city built under the ocean.\n",
      "\n",
      "A futuristic city built under the ocean.\n",
      "\n",
      "\t -> Response by `step_100000_ckp`: Describe a futuristic city built under the ocean. The city would be a city of the future, with\n",
      "\t -> Response by `step_1900000_ckp`: Describe a futuristic city built under the ocean. It is a city that is built under the ocean,\n",
      "\n",
      "== Prompt: Hi, what’s your name?\n",
      "\t -> Response by `SmolLM2-135M`: Hi, what’s your name?\n",
      "\n",
      "Joe: I’m Joe.\n",
      "\n",
      "\n",
      "\t -> Response by `step_100000_ckp`: Hi, what’s your name? I’m Alex. I’m a 16-year-old. I\n",
      "\t -> Response by `step_1900000_ckp`: Hi, what’s your name? I’m a 17-year-old girl. I’m from the\n",
      "\n",
      "== Prompt: Tell me a joke!\n",
      "\t -> Response by `SmolLM2-135M`: Tell me a joke!\n",
      "\n",
      "\"I'm not a fool, I'm a fool!\"\n",
      "\n",
      "\t -> Response by `step_100000_ckp`: Tell me a joke! I'm not sure how to make it funny. I'm not sure how\n",
      "\t -> Response by `step_1900000_ckp`: Tell me a joke! I'm a little bit of a joke maker. I'm a little bit\n",
      "\n",
      "== Prompt: What’s the weather like today in Paris?\n",
      "\t -> Response by `SmolLM2-135M`: What’s the weather like today in Paris?\n",
      "\n",
      "The weather is fine. It’s\n",
      "\t -> Response by `step_100000_ckp`: What’s the weather like today in Paris? What’s the weather like in New York? What’s\n",
      "\t -> Response by `step_1900000_ckp`: What’s the weather like today in Paris? \n",
      "\n",
      "The weather in Paris is very sunny and warm\n"
     ]
    }
   ],
   "source": [
    "test_prompts = [\n",
    "    \"Explain how photosynthesis works in simple terms.\",\n",
    "    \"What would happen if gravity on Earth was suddenly cut in half?\",\n",
    "    \"Compare DNA and RNA in terms of structure and function.\",\n",
    "\n",
    "    \"If a train travels 60 miles per hour for 2.5 hours, how far does it travel?\",\n",
    "    \"What is the next number in the sequence: 2, 4, 8, 16, ?\",\n",
    "    \"A farmer has 17 sheep. All but 9 run away. How many are left?\",\n",
    "\n",
    "    \"Write a Python function that checks if a number is prime.\",\n",
    "    \"Explain what a Python decorator is and give an example.\",\n",
    "    \"Fix this buggy Python code:\\n\\nfor i in range(10)\\n    print(i)\",\n",
    "\n",
    "    \"Summarize the following paragraph in one sentence: 'The climate crisis has accelerated due to industrial emissions and deforestation...'\",\n",
    "    \"Paraphrase: 'Artificial intelligence is changing the way businesses operate.'\",\n",
    "\n",
    "    \"List three steps to make a peanut butter and jelly sandwich.\",\n",
    "    \"Tell me how to reset my Wi-Fi router.\",\n",
    "    \"Translate 'Good morning, how are you?' into Spanish.\",\n",
    "\n",
    "    \"Write a short poem about the moon.\",\n",
    "    \"Continue the story: 'The robot looked at the stars and whispered, \\\"I remember...\\\"'\",\n",
    "    \"Describe a futuristic city built under the ocean.\",\n",
    "\n",
    "    # Conversation / Chatbot Style\n",
    "    \"Hi, what’s your name?\",\n",
    "    \"Tell me a joke!\",\n",
    "    \"What’s the weather like today in Paris?\"\n",
    "]\n",
    "\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    run_inference(evaluated_models,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5e65cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/data-eng/granite_docling/conda-envs/fms-fsdp-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 49,152 49,152\n",
      "Special Tokens:\n",
      "BOS token: <|endoftext|> | ID: 0\n",
      "EOS token: <|endoftext|> | ID: 0\n",
      "PAD token: None | ID: None\n",
      "SEP token: None | ID: None\n",
      "CLS token: None | ID: None\n",
      "UNK token: <|endoftext|> | ID: 0\n",
      "MASK token: None | ID: None\n",
      "\n",
      "All Special Tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|im_start|>', '<|im_end|>', '<repo_name>', '<reponame>', '<file_sep>', '<filename>', '<gh_stars>', '<issue_start>', '<issue_comment>', '<issue_closed>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<jupyter_script>', '<empty_output>']}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check spec.tokens\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "evaluated_models = [\n",
    "    \"/proj/data-eng/granite_docling/experiments/SmolLM2-135M\",\n",
    "    \"/proj/data-eng/granite_docling/experiments/llama165m_granite4tiktoken-2T-pdbs2-8node/hf_model/step_100000_ckp\",\n",
    "    \"/proj/data-eng/granite_docling/experiments/llama165m_granite4tiktoken-2T-pdbs2-8node/hf_model/step_1900000_ckp\"\n",
    "]\n",
    "\n",
    "\n",
    "local_dir = evaluated_models[0]\n",
    "model_name = os.path.basename(os.path.normpath(local_dir))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,} {len(tokenizer):,}\")\n",
    "print(\"Special Tokens:\")\n",
    "print(\"BOS token:\", tokenizer.bos_token, \"| ID:\", tokenizer.bos_token_id)\n",
    "print(\"EOS token:\", tokenizer.eos_token, \"| ID:\", tokenizer.eos_token_id)\n",
    "print(\"PAD token:\", tokenizer.pad_token, \"| ID:\", tokenizer.pad_token_id)\n",
    "print(\"SEP token:\", tokenizer.sep_token, \"| ID:\", tokenizer.sep_token_id)\n",
    "print(\"CLS token:\", tokenizer.cls_token, \"| ID:\", tokenizer.cls_token_id)\n",
    "print(\"UNK token:\", tokenizer.unk_token, \"| ID:\", tokenizer.unk_token_id)\n",
    "print(\"MASK token:\", tokenizer.mask_token, \"| ID:\", tokenizer.mask_token_id)\n",
    "\n",
    "# Alternatively, get a dictionary of all special tokens\n",
    "print(\"\\nAll Special Tokens:\", tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593aaab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 100,352 100,352\n",
      "Special Tokens:\n",
      "BOS token: <|end_of_text|> | ID: 100257\n",
      "EOS token: <|end_of_text|> | ID: 100257\n",
      "PAD token: <|pad|> | ID: 100256\n",
      "SEP token: None | ID: None\n",
      "CLS token: None | ID: None\n",
      "UNK token: <|unk|> | ID: 100269\n",
      "MASK token: None | ID: None\n",
      "\n",
      "All Special Tokens: {'bos_token': '<|end_of_text|>', 'eos_token': '<|end_of_text|>', 'unk_token': '<|unk|>', 'pad_token': '<|pad|>'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "local_dir = evaluated_models[2]\n",
    "model_name = os.path.basename(os.path.normpath(local_dir))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,} {len(tokenizer):,}\")\n",
    "print(\"Special Tokens:\")\n",
    "print(\"BOS token:\", tokenizer.bos_token, \"| ID:\", tokenizer.bos_token_id)\n",
    "print(\"EOS token:\", tokenizer.eos_token, \"| ID:\", tokenizer.eos_token_id)\n",
    "print(\"PAD token:\", tokenizer.pad_token, \"| ID:\", tokenizer.pad_token_id)\n",
    "print(\"SEP token:\", tokenizer.sep_token, \"| ID:\", tokenizer.sep_token_id)\n",
    "print(\"CLS token:\", tokenizer.cls_token, \"| ID:\", tokenizer.cls_token_id)\n",
    "print(\"UNK token:\", tokenizer.unk_token, \"| ID:\", tokenizer.unk_token_id)\n",
    "print(\"MASK token:\", tokenizer.mask_token, \"| ID:\", tokenizer.mask_token_id)\n",
    "\n",
    "# Alternatively, get a dictionary of all special tokens\n",
    "print(\"\\nAll Special Tokens:\", tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56366cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2TokenizerFast'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Info\n",
      "==============\n",
      "Tokenizer class: PreTrainedTokenizerFast\n",
      "Vocab size: 100352\n",
      "Model max length: 1000000000000000019884624838656\n",
      "Padding token: <|pad|> (ID: 100256)\n",
      "BOS token: <|end_of_text|> (ID: 100257)\n",
      "EOS token: <|end_of_text|> (ID: 100257)\n",
      "UNK token: <|unk|> (ID: 100269)\n",
      "\n",
      "Special tokens:\n",
      "  bos_token: <|end_of_text|>\n",
      "  eos_token: <|end_of_text|>\n",
      "  unk_token: <|unk|>\n",
      "  pad_token: <|pad|>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Original granite4 tiktoken:\n",
    "'''\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_path = \"/proj/data-eng/tokenizers/granite4tiktoken\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Print tokenizer information\n",
    "print(\"Tokenizer Info\")\n",
    "print(\"==============\")\n",
    "print(f\"Tokenizer class: {tokenizer.__class__.__name__}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "print(f\"Padding token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")\n",
    "print(\"\\nSpecial tokens:\")\n",
    "for name, tok in tokenizer.special_tokens_map.items():\n",
    "    print(f\"  {name}: {tok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30077d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
