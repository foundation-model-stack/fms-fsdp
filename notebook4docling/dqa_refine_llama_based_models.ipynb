{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c750589b",
   "metadata": {},
   "source": [
    "Notebook to load in a llama-based model from a provided llama_variant. It also helps to inspect and define a new model.\n",
    "\n",
    "For running:\n",
    "    1. conda:\n",
    "        - For SmolLM2-135M: open-instruct-env\n",
    "        - For models defined in Absty fms-fsdp: /proj/data-eng/fsdp/train_env\n",
    "        - For models defined in NEW fms-fsdp-docling: /proj/data-eng/granite_docling/conda-envs/fms-fsdp-env\n",
    "\n",
    "    2.For running, copy this notebook to:\n",
    "        - `/proj/data-eng/fsdp/fms-fsdp` for Absty PPLN (conda env: /proj/data-eng/fsdp/train_env)\n",
    "        - `/proj/data-eng/granite_docling/fms-fsdp-docling/` for New Repo (conda env: /proj/data-eng/granite_docling/conda-envs/fms-fsdp-env)\n",
    "\n",
    "        Otherwise, ModuleNotFoundError: No module named 'fms_fsdp' in running \"from fms_fsdp.utils.config_utils import get_model_config\"\n",
    "\n",
    "        cp ./dqa_refine_llama_based_models.ipynb /proj/data-eng/fsdp/fms-fsdp/\n",
    "        cd /proj/data-eng/fsdp/fms-fsdp/ && code ./dqa_refine_llama_based_models.ipynb\n",
    "        \n",
    "    3. Quick test: \n",
    "        running python -c \"from fms_fsdp.utils.config_utils import get_model_config\"\n",
    "\n",
    "NOTE: \n",
    "    1. Repo where Llama class is defined: \n",
    "            /proj/data-eng/fsdp/foundation-model-stack/fms/models/llama.py\n",
    "    2. Repo where to define NEW Llama-based model: \n",
    "            /proj/data-eng/fsdp/fms-fsdp/fms_fsdp/utils/config_utils.py (starcoder branch, appears to be STALE! -> consider to switch to main)\n",
    "    3. Conda env for showing (in BlueVela):\n",
    "        SmolLM2 model: open-instruct-env\n",
    "        Llama models: /proj/data-eng/fsdp/train_env\n",
    "    \n",
    "\n",
    "CONDA_ENV_PATH=\"/proj/data-eng/fsdp/train_env\"\n",
    "CODE_PATH=\"/proj/data-eng/fsdp/fms-fsdp\"\n",
    "ENV_FILE=\"/proj/data-eng/fsdp/env/train_v01.env\"\n",
    "DATA_PATH=\"/proj/data-eng/fsdp/data/R00\"\n",
    "OUTPUT_PATH=\"/proj/data-eng/fsdp/experiments\"\n",
    "\n",
    "\n",
    "IBM repos for:\n",
    "    + llama.py (where LLaMAConfig() params are offered): https://github.com/foundation-model-stack/foundation-model-stack.git\n",
    "    + config_utils.py (where model_variant is defined): origin  https://github.com/foundation-model-stack/fms-fsdp.git \n",
    "\n",
    "In term of number of params compared between SmolLM2-135M vs llamaVer3_granite4tiktoken: https://ibm-research.slack.com/archives/C08STR7BYKW/p1747684015003109?thread_ts=1747402534.300819&cid=C08STR7BYKW\n",
    "\n",
    "    SmolLM - SmoLM'sEmbdLayer + Granite4-tiktoken'sEmbdlayer = Granite4-tiktoken\n",
    "    134,515,008 - 28,311,552 + 57,802,752  = 164,006,208\n",
    "\n",
    "- both SmolLM2's LlamaDecoderLayer and llamaVer3_granite4tiktoken's LLaMABlock have the exact same parameter count of 3,540,096 per block, their architectures are almost functionally identical.\n",
    "-   SmolLM2: Uses separate linear layers for Q, K, and V projections (q_proj, k_proj, v_proj).\n",
    "    llamaVer3_granite4tiktoken: Uses a single fused linear layer (qkv_fused) for Q, K, and V.\n",
    "    But they are EQUIVALENT since fusing QKV proj into one large matrix multiplication can be faster due to reduced mem bandwidth. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "880f40ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad575777",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "'''\n",
    "conda env:\n",
    "        - For models defined in Absty fms-fsdp: /proj/data-eng/fsdp/train_env\n",
    "        - For models defined in NEW fms-fsdp-docling: /proj/data-eng/granite_docling/conda-envs/fms-fsdp-env\n",
    "'''\n",
    "from fms_fsdp.utils.config_utils import get_model_config\n",
    "from fms.models.llama import LLaMA\n",
    "def print_model(model_variant:str=\"llama2mod_starcoder\"):\n",
    "  c = get_model_config(model_variant)\n",
    "  m = LLaMA(c)\n",
    "  num_trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "  print(f\"== Number of trainable params: {num_trainable_params:,}\") \n",
    "  print(m)\n",
    "  return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2da138",
   "metadata": {},
   "source": [
    "#### 1. NEW models defined in fms-fsdp-docling:\n",
    "- conda: /proj/data-eng/granite_docling/conda-envs/fms-fsdp-env\n",
    "- script: /proj/data-eng/granite_docling/fms-fsdp-docling/fms_fsdp/utils/config_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c86a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Number of trainable params: 164,006,208\n",
      "LLaMA(\n",
      "  (shared): WordEmbedding(\n",
      "    (emb): Embedding(100352, 576)\n",
      "    (head): Linear(in_features=576, out_features=100352, bias=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-29): 30 x LLaMABlock(\n",
      "      (ln): LayerNormParameterized()\n",
      "      (ff_ln): LayerNormParameterized()\n",
      "      (attn): MultiHeadAttention(\n",
      "        (in_proj): FusedQKV(\n",
      "          (qkv_fused): Linear(in_features=576, out_features=960, bias=False)\n",
      "        )\n",
      "        (dense): Linear(in_features=576, out_features=576, bias=False)\n",
      "      )\n",
      "      (ff_sub_layer): GatedLinearUnit(\n",
      "        (wg1_fused): Linear(in_features=576, out_features=3072, bias=False)\n",
      "        (a): SiLU()\n",
      "        (w2): Linear(in_features=1536, out_features=576, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec_norm): LayerNormParameterized()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "_ = print_model(\"llama165m_granite4tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e67950e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== model_variant: llama2mod_starcoder\n",
      "== Number of trainable params: 1,410,959,360\n",
      "LLaMA(\n",
      "  (shared): WordEmbedding(\n",
      "    (emb): Embedding(49152, 2048)\n",
      "    (head): Linear(in_features=2048, out_features=49152, bias=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x LLaMABlock(\n",
      "      (ln): LayerNormParameterized()\n",
      "      (ff_ln): LayerNormParameterized()\n",
      "      (attn): MultiHeadAttention(\n",
      "        (in_proj): FusedQKV(\n",
      "          (qkv_fused): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "        )\n",
      "        (dense): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (ff_sub_layer): GatedLinearUnit(\n",
      "        (w1): Linear(in_features=2048, out_features=5472, bias=False)\n",
      "        (wg): Linear(in_features=2048, out_features=5472, bias=False)\n",
      "        (a): SiLU()\n",
      "        (w2): Linear(in_features=5472, out_features=2048, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec_norm): LayerNormParameterized()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1.4B llama used for Ablation study\n",
    "NOTE: also FusedQKV but normal MLP\n",
    "'''\n",
    "_ = print_model(\"llama2mod_starcoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d5a8c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== rope_interleaved: False\n",
      "== rope_scaling: None\n",
      "== rope_theta: 100000\n",
      "== Tied word embeddings: True\n",
      "== Tied word embeddings via embd weight: True\n",
      "== Number of trainable params of HuggingFaceTB/SmolLM2-135M: 134,515,008\n",
      "== Layers:\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' Oriignal HuggingFaceTB/SmolLM2-135M\n",
    "Need to switch to conda env: open-instruct-env\n",
    "Ref: \n",
    "    github: https://github.com/huggingface/smollm\n",
    "    blog: https://huggingface.co/blog/smollm\n",
    "            https://huggingface.co/HuggingFaceTB/SmolLM2-135M\n",
    "    model + training params:\n",
    "        https://github.com/huggingface/smollm/blob/main/text/pretraining/smollm2/config_smollm2_135M.yaml\n",
    "\n",
    "PP (Sec.4.1): \n",
    "    1. SmolLM1.7B uses LLama2 arch., \n",
    "    2. Tokenizer: from SMolLM1, 49,152 tokens and was trained on a mixture of \n",
    "        70% of FineWeb-edu, \n",
    "        15% Cosmopedia-v2, \n",
    "        8% OpenWebMath, \n",
    "        5% StarCoder Data and \n",
    "        2% StackOverflow.\n",
    "PP (Sec.6): \n",
    "    - Same architecture as SmolLM2-1.7B but use GQA\n",
    "    - trained using the WSD scheduler with 20% decay, lr of 3.0 × 10−3.\n",
    "    - SFT: a filtered version of SmolTalk3, removing complex IF (e.g., function calling)\n",
    "\n",
    "\n",
    "1. Embeddings (model.embed_tokens) (49152, 576): 28,311,552 parameters\n",
    "2. Decoder Layers (model.layers): There are 30 LlamaDecoderLayers.\n",
    "    Each LlamaDecoderLayer consists of:\n",
    "        2.1 self_attn (Q, K, V, O projections): (576 * 576) + (576 * 192) + (576 * 192) + (576 * 576) = 331,776 + 110,592 + 110,592 + 331,776 = 884,736 parameters.\n",
    "        2.2 mlp (gate, up, down projections): (576 * 1536) + (576 * 1536) + (1536 * 576) = 884,736 + 884,736 + 884,736 = 2,654,208 parameters.\n",
    "        2.3 input_layernorm: 576 parameters.\n",
    "        2.4 post_attention_layernorm: 576 parameters.\n",
    "    Total per LlamaDecoderLayer: 884,736 + 2,654,208 + 576 + 576 = 3,540,096 parameters.\n",
    "    Total for 30 layers: 30 * 3,540,096 = 106,202,880 parameters.\n",
    "3. Final Normalization (model.norm): 576 parameters.\n",
    "4. Language Model Head (lm_head): 0 parameters, because tie_word_embeddings: true means its weights are shared with the embedding layer and are not counted separately.\n",
    "\n",
    "Sum: 28,311,552 (embeddings) + 106,202,880 (decoder layers) + 576 (final norm) + 0 (lm_head) = 134,515,008 parameters.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoConfig\n",
    "\n",
    "def inspect_checkpoint(checkpoint:str = \"HuggingFaceTB/SmolLM2-135M\"):\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        checkpoint,\n",
    "        device_map=\"cpu\",  # changed from \"CPU\" to \"cpu\"\n",
    "        torch_dtype=torch.bfloat16\n",
    "        )\n",
    "    # rope_theta:\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "    for attr in dir(config):\n",
    "        if \"rope\" in attr.lower():\n",
    "            print(f\"== {attr}: {getattr(config, attr)}\")\n",
    "\n",
    "    # tie word embd:\n",
    "    print(f\"== Tied word embeddings: {config.tie_word_embeddings}\")\n",
    "    input_embeddings = model.get_input_embeddings()\n",
    "    output_embeddings = model.lm_head\n",
    "    # Check if weights are the same object\n",
    "    tied = input_embeddings.weight.data_ptr() == output_embeddings.weight.data_ptr()\n",
    "    print(f\"== Tied word embeddings via embd weight: {tied}\")\n",
    "    \n",
    "\n",
    "    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"== Number of trainable params of {checkpoint}: {num_trainable_params:,}\")\n",
    "    print(f\"== Layers:\\n{model}\")\n",
    "\n",
    "inspect_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d1726f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== rope_scaling: None\n",
      "== rope_theta: 10000.0\n",
      "== Tied word embeddings: False\n",
      "== Tied word embeddings via embd weight: False\n",
      "== Number of trainable params of /proj/data-eng/granite_docling/experiments/llama165m_granite4tiktoken-2T-8node/hf_model/step_1650000_ckp: 221,808,960\n",
      "== Layers:\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(100352, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=100352, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "inspect_checkpoint(\"/proj/data-eng/granite_docling/experiments/llama165m_granite4tiktoken-2T-8node/hf_model/step_1650000_ckp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115dd2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== rope_scaling: None\n",
      "== rope_theta: 10000.0\n",
      "== Tied word embeddings: False\n",
      "== Tied word embeddings via embd weight: False\n",
      "== Number of trainable params of /proj/data-eng/granite_docling/experiments/00-bkup/WRONG-CONFIG-llama165m_granite4tiktoken-2T-pdbs2-8node/hf_model/step_1900000_ckp: 221,808,960\n",
      "== Layers:\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(100352, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=100352, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "inspect_checkpoint(\"/proj/data-eng/granite_docling/experiments/00-bkup/WRONG-CONFIG-llama165m_granite4tiktoken-2T-pdbs2-8node/hf_model/step_1900000_ckp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d285a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params in Embd layer of SmolLM2: 28,311,552\n",
      "params in Embd layer of llamaVer3_granite4tiktoken: 57,802,752\n"
     ]
    }
   ],
   "source": [
    "print(f\"params in Embd layer of SmolLM2: {49152* 576:,}\")\n",
    "print(f\"params in Embd layer of llamaVer3_granite4tiktoken: {100352* 576:,}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcef7961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== model_variant: llama165m_granite4tiktoken\n",
      "== Number of trainable params: 164,006,208\n",
      "LLaMA(\n",
      "  (shared): WordEmbedding(\n",
      "    (emb): Embedding(100352, 576)\n",
      "    (head): Linear(in_features=576, out_features=100352, bias=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-29): 30 x LLaMABlock(\n",
      "      (ln): LayerNormParameterized()\n",
      "      (ff_ln): LayerNormParameterized()\n",
      "      (attn): MultiHeadAttention(\n",
      "        (in_proj): FusedQKV(\n",
      "          (qkv_fused): Linear(in_features=576, out_features=960, bias=False)\n",
      "        )\n",
      "        (dense): Linear(in_features=576, out_features=576, bias=False)\n",
      "      )\n",
      "      (ff_sub_layer): GatedLinearUnit(\n",
      "        (w1): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (wg): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (a): SiLU()\n",
      "        (w2): Linear(in_features=1536, out_features=576, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec_norm): LayerNormParameterized()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "'''\n",
    "tie_heads=False: 221,808,960\n",
    "tie_heads=True: 164,006,208\n",
    "\n",
    "Layer Description\tTensor Shape\tTotal Number of Parameters\n",
    "    1. WordEmbedding:\t(100352, 576)\t57802752\n",
    "    2. LLaMABlock (one instance):\t3540096\n",
    "    3. dec_norm (LayerNormParameterized)\t(576,)\t576\n",
    "    4. shared.head (Linear) - weights tied to shared.emb\t(576, 100352)\t0\n",
    "\n",
    "TOTAL = 57,802,752(shared.emb)+106,202,880(LLaMABlocks)+576(dec_norm)+0(shared.head) = 164,006,208 parameters.\n",
    "\n",
    "Detail on LLaMABlock (one instance):\t3,540,096\n",
    "\n",
    "Each LLaMABlock consists of:\n",
    "    1. ln (LayerNormParameterized): hidden_size = 576 parameters.\n",
    "    2. ff_ln (LayerNormParameterized): hidden_size = 576 parameters.\n",
    "    3. attn (MultiHeadAttention):\n",
    "        3.1 in_proj.qkv_fused: in_features × out_features = 576×960 = 552,960 parameters.\n",
    "        3.2 dense: in_features × out_features = 576×576 = 331,776 parameters.\n",
    "        -> Total attn parameters: 552,960+331,776 = 884,736 parameters.\n",
    "    4. ff_sub_layer (GatedLinearUnit):\n",
    "        4.1 w1: in_features × out_features = 576×1536 = 884,736 parameters.\n",
    "        4.2 wg: in_features × out_features = 576×1536 = 884,736 parameters.\n",
    "        4.3 w2: in_features × out_features = 1536×576 = 884,736 parameters.\n",
    "        -> Total ff_sub_layer parameters: 884,736+884,736+884,736 = 2,654,208 parameters.\n",
    "\n",
    "    -> Total parameters per LLaMABlock: 576(ln)+576(ff_ln)+884,736(attn)+2,654,208(ff_sub_layer)\n",
    "        = 3,540,096 \n",
    "    -> 30 layers = 30×3,540,096 = 106,202,880\n",
    "\n",
    "Final Summation:\n",
    "\n",
    "\n",
    "\n",
    "Compare to SmolLM above:\n",
    "\n",
    "1. Embeddings (model.embed_tokens) (49152, 576): 28,311,552 parameters\n",
    "2. Decoder Layers (model.layers): There are 30 LlamaDecoderLayers.\n",
    "    Each LlamaDecoderLayer consists of:\n",
    "        2.1 self_attn (Q, K, V, O projections): (576 * 576) + (576 * 192) + (576 * 192) + (576 * 576) = 331,776 + 110,592 + 110,592 + 331,776 = 884,736 parameters.\n",
    "        2.2 mlp (gate, up, down projections): (576 * 1536) + (576 * 1536) + (1536 * 576) = 884,736 + 884,736 + 884,736 = 2,654,208 parameters.\n",
    "        2.3 input_layernorm: 576 parameters.\n",
    "        2.4 post_attention_layernorm: 576 parameters.\n",
    "    Total per LlamaDecoderLayer: 884,736 + 2,654,208 + 576 + 576 = 3,540,096 parameters.\n",
    "    Total for 30 layers: 30 * 3,540,096 = 106,202,880 parameters.\n",
    "3. Final Normalization (model.norm): 576 parameters.\n",
    "4. Language Model Head (lm_head): 0 parameters, because tie_word_embeddings: true means its weights are shared with the embedding layer and are not counted separately.\n",
    "\n",
    "Sum: 28,311,552 (embeddings) + 106,202,880 (decoder layers) + 576 (final norm) + 0 (lm_head) = 134,515,008 parameters.\n",
    "\n",
    "\n",
    "'''\n",
    "llama165m_granite4tiktoken = print_model(\"llama165m_granite4tiktoken\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4703e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4ce7360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== model_variant: llama2mod_starcoder135M_context8K_doclingV01\n",
      "== Number of trainable params: 136,816,000\n",
      "LLaMA(\n",
      "  (shared): WordEmbedding(\n",
      "    (emb): Embedding(49152, 640)\n",
      "    (head): Linear(in_features=640, out_features=49152, bias=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-14): 15 x LLaMABlock(\n",
      "      (ln): LayerNormParameterized()\n",
      "      (ff_ln): LayerNormParameterized()\n",
      "      (attn): MultiHeadAttention(\n",
      "        (in_proj): FusedQKV(\n",
      "          (qkv_fused): Linear(in_features=640, out_features=1920, bias=False)\n",
      "        )\n",
      "        (dense): Linear(in_features=640, out_features=640, bias=False)\n",
      "      )\n",
      "      (ff_sub_layer): GatedLinearUnit(\n",
      "        (w1): Linear(in_features=640, out_features=1712, bias=False)\n",
      "        (wg): Linear(in_features=640, out_features=1712, bias=False)\n",
      "        (a): SiLU()\n",
      "        (w2): Linear(in_features=1712, out_features=640, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec_norm): LayerNormParameterized()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama2mod_starcoder135M_context8K_doclingV01 = print_model(\"llama2mod_starcoder135M_context8K_doclingV01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8598eb17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llama2mod_starcoder135M_context2K_doclingV02 \u001b[38;5;241m=\u001b[39m \u001b[43mprint_model\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama2mod_starcoder135M_context2K_doclingV02\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'print_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "llama2mod_starcoder135M_context2K_doclingV02 = print_model(\"llama2mod_starcoder135M_context2K_doclingV02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf9554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== model_variant: llama2mod_starcoder135M_context2K_doclingV02\n",
      "== Number of trainable params: 136,816,000\n",
      "LLaMA(\n",
      "  (shared): WordEmbedding(\n",
      "    (emb): Embedding(49152, 640)\n",
      "    (head): Linear(in_features=640, out_features=49152, bias=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-14): 15 x LLaMABlock(\n",
      "      (ln): LayerNormParameterized()\n",
      "      (ff_ln): LayerNormParameterized()\n",
      "      (attn): MultiHeadAttention(\n",
      "        (in_proj): FusedQKV(\n",
      "          (qkv_fused): Linear(in_features=640, out_features=1920, bias=False)\n",
      "        )\n",
      "        (dense): Linear(in_features=640, out_features=640, bias=False)\n",
      "      )\n",
      "      (ff_sub_layer): GatedLinearUnit(\n",
      "        (w1): Linear(in_features=640, out_features=1712, bias=False)\n",
      "        (wg): Linear(in_features=640, out_features=1712, bias=False)\n",
      "        (a): SiLU()\n",
      "        (w2): Linear(in_features=1712, out_features=640, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec_norm): LayerNormParameterized()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama2mod_starcoder135M_context2K_doclingV02 = print_model(\"llama2mod_starcoder135M_context2K_doclingV02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80d2e6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== model_variant: llama2mod_granite4tiktoken202M_context8K_doclingV04\n",
      "== Number of trainable params: 202,352,000\n",
      "LLaMA(\n",
      "  (shared): WordEmbedding(\n",
      "    (emb): Embedding(100352, 640)\n",
      "    (head): Linear(in_features=640, out_features=100352, bias=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-14): 15 x LLaMABlock(\n",
      "      (ln): LayerNormParameterized()\n",
      "      (ff_ln): LayerNormParameterized()\n",
      "      (attn): MultiHeadAttention(\n",
      "        (in_proj): FusedQKV(\n",
      "          (qkv_fused): Linear(in_features=640, out_features=1920, bias=False)\n",
      "        )\n",
      "        (dense): Linear(in_features=640, out_features=640, bias=False)\n",
      "      )\n",
      "      (ff_sub_layer): GatedLinearUnit(\n",
      "        (w1): Linear(in_features=640, out_features=1712, bias=False)\n",
      "        (wg): Linear(in_features=640, out_features=1712, bias=False)\n",
      "        (a): SiLU()\n",
      "        (w2): Linear(in_features=1712, out_features=640, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec_norm): LayerNormParameterized()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama2mod_granite4tiktoken202M_context8K_doclingV04 = print_model(\"llama2mod_granite4tiktoken202M_context8K_doclingV04\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a505e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== model_variant: llama2mod_granite4tiktoken138M_context8K_tieheads_doclingV05\n",
      "== Number of trainable params: 138,126,720\n",
      "LLaMA(\n",
      "  (shared): WordEmbedding(\n",
      "    (emb): Embedding(100352, 640)\n",
      "    (head): Linear(in_features=640, out_features=100352, bias=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-14): 15 x LLaMABlock(\n",
      "      (ln): LayerNormParameterized()\n",
      "      (ff_ln): LayerNormParameterized()\n",
      "      (attn): MultiHeadAttention(\n",
      "        (in_proj): FusedQKV(\n",
      "          (qkv_fused): Linear(in_features=640, out_features=1920, bias=False)\n",
      "        )\n",
      "        (dense): Linear(in_features=640, out_features=640, bias=False)\n",
      "      )\n",
      "      (ff_sub_layer): GatedLinearUnit(\n",
      "        (w1): Linear(in_features=640, out_features=1712, bias=False)\n",
      "        (wg): Linear(in_features=640, out_features=1712, bias=False)\n",
      "        (a): SiLU()\n",
      "        (w2): Linear(in_features=1712, out_features=640, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec_norm): LayerNormParameterized()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama2mod_granite4tiktoken138M_context8K_tieheads_doclingV05 = print_model(\"llama2mod_granite4tiktoken138M_context8K_tieheads_doclingV05\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d66c7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== model_variant: llama2mod_granite4tiktoken165M_context8K_tieheads_embdim576nh9nl30_doclingV06\n",
      "== Number of trainable params: 177,277,248\n",
      "LLaMA(\n",
      "  (shared): WordEmbedding(\n",
      "    (emb): Embedding(100352, 576)\n",
      "    (head): Linear(in_features=576, out_features=100352, bias=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-29): 30 x LLaMABlock(\n",
      "      (ln): LayerNormParameterized()\n",
      "      (ff_ln): LayerNormParameterized()\n",
      "      (attn): MultiHeadAttention(\n",
      "        (in_proj): FusedQKV(\n",
      "          (qkv_fused): Linear(in_features=576, out_features=1728, bias=False)\n",
      "        )\n",
      "        (dense): Linear(in_features=576, out_features=576, bias=False)\n",
      "      )\n",
      "      (ff_sub_layer): GatedLinearUnit(\n",
      "        (w1): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (wg): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (a): SiLU()\n",
      "        (w2): Linear(in_features=1536, out_features=576, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec_norm): LayerNormParameterized()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama2mod_granite4tiktoken165M_context8K_tieheads_embdim576nh9nl30_doclingV06 = print_model(\"llama2mod_granite4tiktoken165M_context8K_tieheads_embdim576nh9nl30_doclingV06\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6579bb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== model_variant: llama165m_granite4tiktoken\n",
      "== Number of trainable params: 164,006,208\n",
      "LLaMA(\n",
      "  (shared): WordEmbedding(\n",
      "    (emb): Embedding(100352, 576)\n",
      "    (head): Linear(in_features=576, out_features=100352, bias=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-29): 30 x LLaMABlock(\n",
      "      (ln): LayerNormParameterized()\n",
      "      (ff_ln): LayerNormParameterized()\n",
      "      (attn): MultiHeadAttention(\n",
      "        (in_proj): FusedQKV(\n",
      "          (qkv_fused): Linear(in_features=576, out_features=960, bias=False)\n",
      "        )\n",
      "        (dense): Linear(in_features=576, out_features=576, bias=False)\n",
      "      )\n",
      "      (ff_sub_layer): GatedLinearUnit(\n",
      "        (w1): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (wg): Linear(in_features=576, out_features=1536, bias=False)\n",
      "        (a): SiLU()\n",
      "        (w2): Linear(in_features=1536, out_features=576, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec_norm): LayerNormParameterized()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama165m_granite4tiktoken = print_model(\"llama165m_granite4tiktoken\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
